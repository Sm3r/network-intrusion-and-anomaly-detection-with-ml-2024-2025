{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2916704",
   "metadata": {},
   "source": [
    "# Anomaly detection with Linear Regression\n",
    "In this laboratory, we will use a Linear Regression to predict the average Inter Arrival Time (IAT) of packets and to spot anomalies. In particular, we make our model learning the average IAT of benign flows and we predict the IAT of new traffic. If the squared error between the prediction and the actual value is higher than a threshold, we label the sample (flow) as an anomaly. After that, we compute the accuracy scores.\n",
    "We will train a linear regression model on a dataset of benign traffic, and then we will test the trained model with DDoS attack traffic.\n",
    "\n",
    "We will use a dataset of benign and various DDoS attacks from the CIC-DDoS2019 dataset (https://www.unb.ca/cic/datasets/ddos-2019.html).\n",
    "The network traffic has been previously pre-processed in a way that packets are grouped in bi-directional traffic flows using the 5-tuple (source IP, destination IP, source Port, destination Port, protocol). Each flow is represented with 21 packet-header features computed from max 1000 packets:\n",
    "\n",
    "| Feature nr.         | Feature Name |\n",
    "|---------------------|---------------------|\n",
    "| 00 | timestamp (mean IAT) | \n",
    "| 01 | packet_length (mean)| \n",
    "| 02 | IP_flags_df (sum) |\n",
    "| 03 | IP_flags_mf (sum) |\n",
    "| 04 | IP_flags_rb (sum) | \n",
    "| 05 | IP_frag_off (sum) |\n",
    "| 06 | protocols (mean) |\n",
    "| 07 | TCP_length (mean) |\n",
    "| 08 | TCP_flags_ack (sum) |\n",
    "| 09 | TCP_flags_cwr (sum) |\n",
    "| 10 | TCP_flags_ece (sum) |\n",
    "| 11 | TCP_flags_fin (sum) |\n",
    "| 12 | TCP_flags_push (sum) |\n",
    "| 13 | TCP_flags_res (sum) |\n",
    "| 14 | TCP_flags_reset (sum) |\n",
    "| 15 | TCP_flags_syn (sum) |\n",
    "| 16 | TCP_flags_urg (sum) |\n",
    "| 17 | TCP_window_size (mean) |\n",
    "| 18 | UDP_length (mean) |\n",
    "| 19 | ICMP_type (mean) |\n",
    "| 20 | Packets (counter)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a046372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import h5py\n",
    "import sys\n",
    "import copy\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "import logging\n",
    "import seaborn as sns\n",
    "from util_functions import *\n",
    "from IPython.display import Image, display\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "OUTPUT_FILE = \"./rf_tree\"\n",
    "BENIGN_FOLDER = \"./DOS2019/benign\"\n",
    "ATTACK_DNS = \"./DOS2019/dns\"\n",
    "ATTACK_WEBDDOS = \"./DOS2019/webddos\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'figure.figsize': (12.0, 8.0)})\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "SEED=1\n",
    "feature_names = get_feature_names()\n",
    "target_names = ['benign', 'dns',  'syn', 'udplag', 'webddos'] #IMPORTANT: when adding new classes, maintain the alphabetical order\n",
    "target_names_full = ['benign', 'dns', 'ldap', 'mssql', 'netbios', 'ntp', 'portmap', 'snmp', 'ssdp', 'syn', 'tftp', 'udp', 'udplag', 'webddos'] # we use this to match class names with the class numbers returned by the RF\n",
    "\n",
    "X_train_benign, y_train_benign = load_dataset(BENIGN_FOLDER + \"/*\" + '-train.hdf5')\n",
    "X_val_benign, y_val_benign = load_dataset(BENIGN_FOLDER + \"/*\" + '-val.hdf5')\n",
    "X_test_dns, y_test_dns = load_dataset(ATTACK_DNS + \"/*\" + '-test.hdf5')\n",
    "X_test_webddos, y_test_webddos = load_dataset(ATTACK_WEBDDOS + \"/*\" + '-test.hdf5')\n",
    "\n",
    "X_test_attack = np.concatenate((X_test_dns,X_test_webddos))\n",
    "y_test_attack = np.concatenate((y_test_dns,y_test_webddos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ebe28",
   "metadata": {},
   "source": [
    "# Dataset for regression problems\n",
    "In the next cell, we modify the dataset to make it suitable for regression problems. In particular, we will use one of the features as target value (e.g, the average inter-arrival time of packets, or the average packet size). The selected feature is also removed from the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfc50c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_INDEX = 0 # index of the feature used as value to predict. 0 = averae IAT, 1 = average packet size of the flow\n",
    "\n",
    "y_reg_train_benign = X_train_benign[:,Y_INDEX]\n",
    "X_reg_train_benign = np.delete(X_train_benign, Y_INDEX, axis=1)\n",
    "\n",
    "y_reg_val_benign = X_val_benign[:,Y_INDEX]\n",
    "X_reg_val_benign = np.delete(X_val_benign, Y_INDEX, axis=1)\n",
    "\n",
    "y_reg_test_attack = X_test_attack[:,Y_INDEX]\n",
    "X_reg_test_attack = np.delete(X_test_attack, Y_INDEX, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7e4b0",
   "metadata": {},
   "source": [
    "# Training the Linear Regressor\n",
    "We train the Linear Regressor to predict the average packet size of the benign flows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_reg_train_benign, y_reg_train_benign)\n",
    "\n",
    "print(\"Bias: \", lr.intercept_) # bias\n",
    "print(\"Weights: \", lr.coef_) # weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a20fb1",
   "metadata": {},
   "source": [
    "# Validating the Linear Regressor with benign traffic\n",
    "Now the Linear regressor is trained. We can use it to make predictions on unseen data. We first start with the validation set of benign traffic, which allows us to understand what is the average error of the prediction on it. \n",
    "This MSE and the standard deviation of the squared errors will be used to define the threshold for anomaly detection. We add the standard deviation to minimise false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feed3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg_pred_benign = lr.predict(X_reg_val_benign)\n",
    "\n",
    "benign_mse = mean_squared_error(y_reg_val_benign,y_reg_pred_benign)\n",
    "print(\"MSE measures on the benign validation set: \", benign_mse)\n",
    "\n",
    "benign_squared_errors = ((y_reg_val_benign-y_reg_pred_benign)**2)\n",
    "print(\"Standard deviation of squared errors: \", benign_squared_errors.std())\n",
    "\n",
    "# Define the threshold as the MSE+std_dev\n",
    "attack_threshold = benign_mse + benign_squared_errors.std()\n",
    "print (\"Anomaly Threshold: \", attack_threshold)\n",
    "\n",
    "# Plot the squared error distribution\n",
    "plt.hist(benign_squared_errors, color = 'blue', edgecolor = 'black',bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b862e1",
   "metadata": {},
   "source": [
    "# Testing the Linear Regressor with DDoS attack traffic\n",
    "We test the Linear regressor on attack data samples. These samples have been generated by using DDoS attacks from the CIC-DDoS2019 dataset. We first measure the MSE and then we print the accuracy metrics obtained with the anomaly threshold computed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg_pred_attack = lr.predict(X_reg_test_attack)\n",
    "\n",
    "attack_mse = mean_squared_error(y_reg_test_attack,y_reg_pred_attack)\n",
    "print(\"MSE measures on the attack test set: \", attack_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104956de",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_squared_errors = ((y_reg_test_attack-y_reg_pred_attack)**2)\n",
    "\n",
    "# Plot the squared error distribution\n",
    "plt.hist(attack_squared_errors, color = 'blue', edgecolor = 'black',bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f68282",
   "metadata": {},
   "source": [
    "# Accuracy metrics\n",
    "We compute the accuracy metrics on the test sets of benign and attack traffic using the threshold computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaded6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "tn_fp = [True if err < attack_threshold else False for err in benign_squared_errors]\n",
    "tp_fn = [True if err > attack_threshold else False for err in attack_squared_errors]\n",
    "\n",
    "tn = sum(tn_fp)\n",
    "tp = sum(tp_fn)\n",
    "fp = len(tn_fp) - tn\n",
    "fn = len(tp_fn) - tp\n",
    "\n",
    "# Display the confusion matrix\n",
    "conf_matrix = np.array([[tn,fp],[fn,tp]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbaab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "tpr = tp/(tp+fn) # recall\n",
    "ppv = tp/(tp+fp) # precision\n",
    "fpr = fp/(fp+tn) # false positive rate\n",
    "fnr = fn/(fn+tp) # false negative rate\n",
    "f1_score = 2*(tpr*ppv)/(tpr+ppv)\n",
    "\n",
    "\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"F1 Score: \", f1_score)\n",
    "print (\"FPR: \", fpr)\n",
    "print (\"FNR: \", fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954bee6f",
   "metadata": {},
   "source": [
    "# Linear regression with polynomial features\n",
    "You can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bd987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_poly now contains the original feature of X plus the square of this feature\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly_train = poly_features.fit_transform(X_reg_train_benign)\n",
    "\n",
    "lr = LinearRegression() \n",
    "lr.fit(X_poly_train, y_reg_train_benign)\n",
    "print(lr.intercept_, lr.coef_) # print bias and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50b4d8",
   "metadata": {},
   "source": [
    "# Validating the Linear Regressor with polynomial features using the benign traffic\n",
    "We can use the new regressor to make predictions on the validation set. Also in this case, we use the results of this test to set the anomaly threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_val = poly_features.fit_transform(X_reg_val_benign)\n",
    "y_reg_pred_benign = lr.predict(X_poly_val)\n",
    "\n",
    "benign_mse = mean_squared_error(y_reg_val_benign,y_reg_pred_benign)\n",
    "print(\"MSE measured on the benign validation set: \", benign_mse)\n",
    "\n",
    "benign_squared_errors = ((y_reg_val_benign-y_reg_pred_benign)**2)\n",
    "print(\"Standard deviation of squared errors: \", benign_squared_errors.std())\n",
    "\n",
    "# Define the threshold as the MSE+std_dev\n",
    "attack_threshold = benign_mse + benign_squared_errors.std()\n",
    "print (\"Anomaly Threshold: \", attack_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773211de",
   "metadata": {},
   "source": [
    "# Testing the Linear Regressor with DDoS attack traffic\n",
    "We test the Linear regressor on attack data samples. These samples have been generated by using DDoS attacks from the CIC-DDoS2019 dataset. We first measure the MSE and then we print the accuracy metrics obtained with the anomaly threshold computed in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55403bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_test_attack = poly_features.fit_transform(X_reg_test_attack)\n",
    "y_reg_pred_attack = lr.predict(X_poly_test_attack)\n",
    "print (y_reg_pred_attack)\n",
    "\n",
    "attack_mse = mean_squared_error(y_reg_test_attack,y_reg_pred_attack)\n",
    "print(\"MSE measured on the benign test set: \", attack_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28638e9f",
   "metadata": {},
   "source": [
    "# Accuracy metrics\n",
    "We compute the accuracy metrics on the test sets of benign and attack traffic using the threshold computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96484c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "tn_fp = [True if err < attack_threshold else False for err in benign_squared_errors]\n",
    "tp_fn = [True if err > attack_threshold else False for err in attack_squared_errors]\n",
    "\n",
    "tn = sum(tn_fp)\n",
    "tp = sum(tp_fn)\n",
    "fp = len(tn_fp) - tn\n",
    "fn = len(tp_fn) - tp\n",
    "\n",
    "# Display the confusion matrix\n",
    "conf_matrix = np.array([[tn,fp],[fn,tp]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "tpr = tp/(tp+fn) # recall\n",
    "ppv = tp/(tp+fp) # precision\n",
    "fpr = fp/(fp+tn) # false positive rate\n",
    "fnr = fn/(fn+tp) # false negative rate\n",
    "f1_score = 2*(tpr*ppv)/(tpr+ppv)\n",
    "\n",
    "\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print (\"F1 Score: \", f1_score)\n",
    "print (\"FPR: \", fpr)\n",
    "print (\"FNR: \", fnr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
