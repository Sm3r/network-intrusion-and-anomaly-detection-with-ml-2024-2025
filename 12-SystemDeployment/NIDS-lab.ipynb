{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Deployment\n",
    "This laboratory encompasses the entire life-cycle of a NIDS. It spans from the ANN model design, through the phases of model training and testing, culminating in the deployment onto the target machine. Follow the steps of the laboratory.\n",
    "\n",
    "## Model implementation\n",
    "1. Implement an MLP model from scratch\n",
    "2. Define the ranges of hyperparameters and execute training\n",
    "3. Execute hyperparameter tuning\n",
    "4. Test the trained model on the test set and on one or more pcap files using the notebook\n",
    "\n",
    "## Model deployment\n",
    "1. Export the notebook to a stand-alone Python script.\n",
    "2. Execute the script using the arguments ```--model``` and ```--predict``` to indicate the paths to the trained model and to the folder with the test set respectively. \n",
    "3. Execute the script using the arguments ```--model``` and ```--predict_live``` to indicate the paths to the trained model and to a ```pcap``` file respectively.\n",
    "4. \n",
    "    (a) Execute the script using the arguments ```--model``` and ```--predict_live lo```, where ```lo``` is the loopback interface (special network interface that the system uses to communicate with itself). \n",
    "    (b) In another terminal, simulate a network attack by injecting the traffic traces in the ```DOS2019_Binary_5_Attacks_PCAPs``` to the ```lo``` interface using ```tcpreplay -i lo traffic_trace.pcap```\n",
    "\n",
    "You will use a dataset of benign and various DDoS attacks from the CIC-DDoS2019 dataset (https://www.unb.ca/cic/datasets/ddos-2019.html).\n",
    "The network traffic has been previously pre-processed in a way that packets are grouped in bi-directional traffic flows using the 5-tuple (source IP, destination IP, source Port, destination Port, protocol). Each flow is represented with 21 packet-header features computed from max 1000 packets:\n",
    "\n",
    "| Feature nr.         | Feature Name |\n",
    "|---------------------|---------------------|\n",
    "| 00 | timestamp (mean IAT) | \n",
    "| 01 | packet_length (mean)| \n",
    "| 02 | IP_flags_df (sum) |\n",
    "| 03 | IP_flags_mf (sum) |\n",
    "| 04 | IP_flags_rb (sum) | \n",
    "| 05 | IP_frag_off (sum) |\n",
    "| 06 | protocols (mean) |\n",
    "| 07 | TCP_length (mean) |\n",
    "| 08 | TCP_flags_ack (sum) |\n",
    "| 09 | TCP_flags_cwr (sum) |\n",
    "| 10 | TCP_flags_ece (sum) |\n",
    "| 11 | TCP_flags_fin (sum) |\n",
    "| 12 | TCP_flags_push (sum) |\n",
    "| 13 | TCP_flags_res (sum) |\n",
    "| 14 | TCP_flags_reset (sum) |\n",
    "| 15 | TCP_flags_syn (sum) |\n",
    "| 16 | TCP_flags_urg (sum) |\n",
    "| 17 | TCP_window_size (mean) |\n",
    "| 18 | UDP_length (mean) |\n",
    "| 19 | ICMP_type (mean) |\n",
    "| 20 | Packets (counter)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import pyshark\n",
    "import numpy as np\n",
    "import pprint\n",
    "from scipy.stats import uniform, randint\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential,load_model, save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv2D, GlobalMaxPooling2D, Flatten, Dropout\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from lucid_dataset_parser import *\n",
    "from util_functions import *\n",
    "\n",
    "# We need the following to get around “RuntimeError: This event loop is already running” when using Pyshark within Jupyter notebooks.\n",
    "# Not needed in stand-alone Python projects\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  \n",
    "\n",
    "EPOCHS = 100\n",
    "TEST_ITERATIONS=100\n",
    "\n",
    "# disable GPUs for test reproducibility\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument parser\n",
    "The following cell defines the arguments that are accepted by the NIDS. The arguments ```--train``` and ```--predict``` can be used to indicate the folder with the dataset. The script will loaf the ```hdf5``` files for training and testing respectively. The argument ```--predict_live``` can be used to perform predictions on live traffic captured from a network interface (e.g., ```eth0``` or ```lo```) or to make prediction using a pre-recorded traffic trace (e.g, ```ddos-chunk.pcap```). In both cases the argument is a string. In the first case, it is the name of the interface, in the second case, the path to the ```pcap``` file. The argument ```--model``` indicates the path to a trained model that will be used to make predictions (```--predict``` or ```predict_live```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"A DL-based NIDS for DDoS attack detection\")\n",
    "args = parser.parse_args(args=[])\n",
    "parser.add_argument('-t', '--train', nargs='?', type=str,  default=None, help=\"Start the training process\")\n",
    "parser.add_argument('-p', '--predict', nargs='?', type=str,  default=None, help=\"Perform a prediction on pre-preprocessed data\")\n",
    "parser.add_argument('-pl', '--predict_live', nargs='?', type=str, default=None, help='Perform a prediction on live traffic or on a pre-recorded traffic trace in pcap format')\n",
    "parser.add_argument('-m', '--model', type=str, default = None, help='File containing the model in h5 format')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(\"see all args:\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(Y_true, Y_pred, model_name, data_source, prediction_time):\n",
    "    ddos_rate = '{:04.3f}'.format(sum(Y_pred) / Y_pred.shape[0])\n",
    "\n",
    "    if Y_true is not None and len(Y_true.shape) > 0:  # if we have the labels, we can compute the classification accuracy\n",
    "        Y_true = Y_true.reshape((Y_true.shape[0], 1))\n",
    "        accuracy = accuracy_score(Y_true, Y_pred)\n",
    "\n",
    "        f1 = f1_score(Y_true, Y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred, labels=[0, 1]).ravel()\n",
    "        tnr = tn / (tn + fp)\n",
    "        fpr = fp / (fp + tn)\n",
    "        fnr = fn / (fn + tp)\n",
    "        tpr = tp / (tp + fn)\n",
    "\n",
    "        row = {'Model': model_name, 'Time': '{:04.3f}'.format(prediction_time),\n",
    "               'Samples': Y_pred.shape[0], 'DDOS%': ddos_rate, 'Accuracy': '{:05.4f}'.format(accuracy), 'F1Score': '{:05.4f}'.format(f1),\n",
    "               'TPR': '{:05.4f}'.format(tpr), 'FPR': '{:05.4f}'.format(fpr), 'TNR': '{:05.4f}'.format(tnr), 'FNR': '{:05.4f}'.format(fnr), 'Source': data_source}\n",
    "\n",
    "    pprint.pprint(row, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement an MLP model\n",
    "Finalise the model by using all the four arguments of ```create_model```. You will use ALL these arguments to tune the model afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer=SGD, dense_layers=4, hidden_units=2, learning_rate = 0.001):\n",
    "    model = Sequential(name  = \"mlp\")\n",
    "    ### Add YOUR CODE HERE ###\n",
    "    \n",
    "    ##########################\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on static test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset_path, model_path):\n",
    "    if dataset_path is not None:\n",
    "        X_test, y_test = load_dataset(dataset_path + \"/*\" + '-test.hdf5')\n",
    "\n",
    "        if model_path == None or model_path.endswith('.h5') == False:\n",
    "                print (\"No valid model specified!\")\n",
    "                exit(-1)\n",
    "\n",
    "        if model_path is not None:\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path) \n",
    "            return\n",
    "\n",
    "        pt0 = time.time()\n",
    "        for i in range(TEST_ITERATIONS):\n",
    "            Y_pred = np.squeeze(model.predict(X_test, batch_size=16) > 0.5,axis=1)\n",
    "        pt1 = time.time()\n",
    "        prediction_time = pt1 - pt0\n",
    "\n",
    "        report_results(np.squeeze(y_test), Y_pred,  model.name, '', prediction_time/TEST_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on live traffic\n",
    "Complete the following cell by printing the identifiers of the DDoS flows (```flow_ids```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_live(source,model_path):\n",
    "    if source is not None:\n",
    "        if source.endswith('.pcap'):\n",
    "            pcap_file = source\n",
    "            cap = pyshark.FileCapture(pcap_file)\n",
    "            data_source = pcap_file.split('/')[-1].strip()\n",
    "        else:\n",
    "            cap =  pyshark.LiveCapture(interface=source)\n",
    "            data_source = args.predict_live\n",
    "\n",
    "        print (\"Prediction on network traffic from: \", source)\n",
    "\n",
    "        if model_path is not None:\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path) \n",
    "            return\n",
    "\n",
    "        # load the labels, if available\n",
    "        labels = parse_labels('DOS2019')\n",
    "\n",
    "        mins, maxs = static_min_max(flatten=True,time_window=10,max_flow_len=1000)\n",
    "\n",
    "        while (True):\n",
    "            samples = process_live_traffic(cap, 'DOS2019', labels, max_flow_len=1000, traffic_type=\"all\")\n",
    "            if len(samples) > 0:\n",
    "                X,Y_true,flow_ids = dataset_to_list_of_fragments(samples)\n",
    "                X_flatten = flatten_samples(X)\n",
    "                X = np.array(normalize(X_flatten, mins, maxs))\n",
    "                if labels is not None:\n",
    "                    Y_true = np.array(Y_true)\n",
    "                else:\n",
    "                    Y_true = None\n",
    "                \n",
    "                pt0 = time.time()\n",
    "                Y_pred = np.squeeze(model.predict(X, batch_size=2048) > 0.5,axis=1)\n",
    "                pt1 = time.time()\n",
    "                prediction_time = pt1 - pt0\n",
    "\n",
    "                report_results(np.squeeze(Y_true), Y_pred,  model.name, '', prediction_time)\n",
    "                \n",
    "                ### ADD YOUR CODE HERE ###\n",
    "\n",
    "                ##########################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "Complete the code of the cell below by implementing random search on the 4 hyperparameters of the ```create_model``` method above. Use k-fold cross-validation with ```k=2``` and a maximum of ```10``` iterations for the random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_path, model_path):\n",
    "    if dataset_path is not None:\n",
    "        X_train, y_train = load_dataset(dataset_path + \"/*\" + '-train.hdf5')\n",
    "        X_val, y_val = load_dataset(dataset_path + \"/*\" + '-val.hdf5')\n",
    "\n",
    "        param_dist = {\n",
    "            ### ADD YOUR CODE HERE ###\n",
    "\n",
    "            ##########################\n",
    "        }\n",
    "\n",
    "        model = KerasClassifier(build_fn=create_model, batch_size=100, verbose=1)\n",
    "\n",
    "        ### ADD YOUR CODE HERE ###\n",
    "        random_search = ### ADD YOUR CODE HERE ###\n",
    "        early_stopping = ### ADD YOUR CODE HERE ###\n",
    "        random_search_result = ### ADD YOUR CODE HERE ###\n",
    "        ##########################\n",
    "\n",
    "        # Print the best parameters and corresponding accuracy\n",
    "        print(\"Best parameters found: \", random_search_result.best_params_)\n",
    "        print(\"Best cross-validated accuracy: {:.2f}\".format(random_search_result.best_score_))\n",
    "\n",
    "        # Save the best model\n",
    "        best_model = random_search.best_estimator_.model\n",
    "        if model_path is not None:\n",
    "            save_model(best_model,model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path)\n",
    "            print (\"Model saved as: \" + './nids_model.h5')\n",
    "            save_model(best_model,'./nids_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model\n",
    "Train you model by executing the method above with appropriate arguments (```args....``` see the ```argparse``` cell above). This will prepare the code for the stand-alone script, which will accept arguments from the command line. \n",
    "If you want to train the model in this notebook for testing purposes, you can first call the ```train``` method with static dataset path (```'./DOS2019_Binary_5_Attacks_MLP'```) and model path (e.g., ```'./nids_model.h5'```). \n",
    "\n",
    "**NOTE1**: these arguments are strings, do not forget the two paths between ```''```.\n",
    "**NOTE2**: Before exporting the notebook to a Python script, remember to replace the static parameters with the previous```args....```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "In the following cell, add the code to make prediction with the model saved before on the test set. \n",
    "If you want to test the model in this notebook, you can first call the ```predict``` method with static dataset path (```'./DOS2019_Binary_5_Attacks_MLP'```) and model path (e.g., ```'./nids_model.h5'```). \n",
    "\n",
    "**NOTE1**: these arguments are strings, do not forget the two paths between ```''```.\n",
    "**NOTE2**: Before exporting the notebook to a Python script, remember to replace the static parameters with the previous```args....```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "\n",
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using a pcap file\n",
    "In the following cell, add the code to make prediction with the model saved before on a pcap file. \n",
    "If you want to test the model in this notebook, you can first call the ```predict_live``` method using the path to a ```pcap``` file (e.g., ```'./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk.pcap'```) and model path (e.g., ```'./nids_model.h5'```). \n",
    "\n",
    "**NOTE1**: these arguments are strings, do not forget the two paths between ```''```.\n",
    "**NOTE2**: Before exporting the notebook to a Python script, remember to replace the static parameters with the previous```args....```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on a pcap file\n",
    "\n",
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "##########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
