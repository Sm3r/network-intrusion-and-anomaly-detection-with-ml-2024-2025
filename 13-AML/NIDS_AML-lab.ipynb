{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Machine Learning attack against a DL-based NIDS\n",
    "In this experiment, our primary focus is to assess the robustness of DL-based NIDSs against evasion attacks using AML. Specifically, we evaluate MLP-based and CNN-based NIDSs under two conditions: (1) unperturbed attack and benign network traffic, and (2) manipulated network traffic.\n",
    "\n",
    "The objective of this study is to analyze how network traffic manipulation impacts the accuracy of the NIDS and to explore how attackers can exploit weaknesses in NIDSs to execute successful attacks.\n",
    "\n",
    "Both models are designed for binary classification of the network traffic as benign or malicious. Therefore, they return a value between 0 and 1, which is the probability of the input flow of being malicious. \n",
    "\n",
    "\n",
    "| <img src=\"./mlp-cnn.png\" width=\"100%\">  |\n",
    "|--|\n",
    "| Sample MLP and CNN architectures|\n",
    "\n",
    "\n",
    "The laboratory is divided into two phases:\n",
    "-  **Phase 1**: MLP-based NIDS\n",
    "    - train a pre-defined MLP model and test it using the test set and an unperturbed traffic trace [ddos-chunk-short.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short.pcap)\n",
    "    - use the modified traces to evade the NIDS:\n",
    "        - IAT set to 0.5 seconds [ddos-chunk-short-IAT0.5.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-IAT0.5.pcap5.pcap)\n",
    "        - Random packet length [ddos-chunk-short-PACKETLEN.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-PACKETLEN.pcap)\n",
    "        - Random TCP Window size [ddos-chunk-short-WINSIZE.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-WINSIZE.pcap)\n",
    "-  **Phase 2**: CNN-based NIDS\n",
    "    - Implement a CNN-based NIDS e define the relevant hyper-parameters\n",
    "    - train the CNN model and test it using the test set and an unperturbed traffic trace [ddos-chunk-short.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short.pcap)\n",
    "    - use the modified traces to evade the NIDS:\n",
    "        - IAT set to 0.5 seconds [ddos-chunk-short-IAT0.5.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-IAT0.5.pcap5.pcap)\n",
    "        - Random packet length [ddos-chunk-short-PACKETLEN.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-PACKETLEN.pcap)\n",
    "        - Random TCP Window size [ddos-chunk-short-WINSIZE.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-WINSIZE.pcap)\n",
    "\n",
    "**NOTE:** Select the appropriate model by setting variable ```MODEL_TYPE``` in the first cell of this notebook.\n",
    "\n",
    "## Dataset\n",
    "We will use a dataset of benign and various DDoS attacks from the CIC-DDoS2019 dataset (https://www.unb.ca/cic/datasets/ddos-2019.html).\n",
    "The network traffic has been previously pre-processed in a way that packets are grouped in bi-directional traffic flows using the 5-tuple (source IP, destination IP, source Port, destination Port, protocol). \n",
    "\n",
    "Each flow is represented either as a vector of 21 statistical packet-level features (MLP model) or as a 10x20 array (CNN model) of raw packet-level features, where the rows are the packets of the flow in chronological order, while each column is a packet-level feature. \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Features for the MLP model</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Feature nr. </th> <th>Feature Name</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>00 </td> <td>timestamp (mean IAT)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>01 </td> <td>packet_length (mean)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>02 </td> <td>IP_flags_df (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>03 </td> <td>IP_flags_mf (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>04 </td> <td>IP_flags_rb (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>05 </td> <td>IP_frag_off (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>06 </td> <td>protocols (mean)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>07 </td> <td>TCP_length (mean)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>08 </td> <td>TCP_flags_ack (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>09 </td> <td>TCP_flags_cwr (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10 </td> <td>TCP_flags_ecn (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11 </td> <td>TCP_flags_fin (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12 </td> <td>TCP_flags_push (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13 </td> <td>TCP_flags_res (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14 </td> <td>TCP_flags_reset (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15 </td> <td>TCP_flags_syn (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16 </td> <td>TCP_flags_urg (sum)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17 </td> <td>TCP_window_size (mean)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18 </td> <td>UDP_length (mean)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19 </td> <td>ICMP_type (mean)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20 </td> <td>Packets (counter)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Features for the CNN model (columns of the 100x20 array)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Feature nr. </th> <th>Feature Name</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>00 </td> <td>timestamp (IAT)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>01 </td> <td>packet_length (bytes)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>02 </td> <td>IP_flags_df (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>03 </td> <td>IP_flags_mf (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>04 </td> <td>IP_flags_rb (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>05 </td> <td>IP_frag_off (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>06 </td> <td>protocols (integer)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>07 </td> <td>TCP_length (bytes)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>08 </td> <td>TCP_flags_ack (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>09 </td> <td>TCP_flags_cwr (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10 </td> <td>TCP_flags_ecn (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11 </td> <td>TCP_flags_fin (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12 </td> <td>TCP_flags_push (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13 </td> <td>TCP_flags_res (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14 </td> <td>TCP_flags_reset (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15 </td> <td>TCP_flags_syn (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16 </td> <td>TCP_flags_urg (0/1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17 </td> <td>TCP_window_size (bytes)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18 </td> <td>UDP_length (bytes)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>19 </td> <td>ICMP_type (code)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import pyshark\n",
    "import numpy as np\n",
    "import pprint\n",
    "from scipy.stats import uniform, randint\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential,load_model, save_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv2D, GlobalMaxPooling2D, Flatten, Dropout\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from lucid_dataset_parser import *\n",
    "from util_functions import *\n",
    "\n",
    "# We need the following to get around “RuntimeError: This event loop is already running” when using Pyshark within Jupyter notebooks.\n",
    "# Not needed in stand-alone Python projects\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  \n",
    "\n",
    "EPOCHS = 100\n",
    "TEST_ITERATIONS=100\n",
    "\n",
    "### SELECT THE MODEL_TYPE HERE ('MLP' or 'CNN') ###\n",
    "MODEL_TYPE = 'MLP' \n",
    "###################################################\n",
    "\n",
    "# disable GPUs for test reproducibility\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument parser\n",
    "The following cell defines the arguments that are accepted by the NIDS. The arguments ```--train``` and ```--predict``` can be used to indicate the folder with the dataset. The script will loaf the ```hdf5``` files for training and testing respectively. The argument ```--predict_live``` can be used to perform predictions on live traffic captured from a network interface (e.g., ```eth0``` or ```lo```) or to make prediction using a pre-recorded traffic trace (e.g, ```ddos-chunk.pcap```). In both cases the argument is a string. In the first case, it is the name of the interface, in the second case, the path to the ```pcap``` file. The argument ```--model``` indicates the path to a trained model that will be used to make predictions (```--predict``` or ```predict_live```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"A DL-based NIDS for DDoS attack detection\")\n",
    "args = parser.parse_args(args=[])\n",
    "parser.add_argument('-t', '--train', nargs='?', type=str,  default=None, help=\"Start the training process\")\n",
    "parser.add_argument('-p', '--predict', nargs='?', type=str,  default=None, help=\"Perform a prediction on pre-preprocessed data\")\n",
    "parser.add_argument('-pl', '--predict_live', nargs='?', type=str, default=None, help='Perform a prediction on live traffic or on a pre-recorded traffic trace in pcap format')\n",
    "parser.add_argument('-m', '--model', type=str, default = None, help='File containing the model in h5 format')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(\"see all args:\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(Y_true, Y_pred, model_name, data_source, prediction_time):\n",
    "    ddos_rate = '{:04.3f}'.format(sum(Y_pred) / Y_pred.shape[0])\n",
    "\n",
    "    if Y_true is not None and len(Y_true.shape) > 0:  # if we have the labels, we can compute the classification accuracy\n",
    "        Y_true = Y_true.reshape((Y_true.shape[0], 1))\n",
    "        accuracy = accuracy_score(Y_true, Y_pred)\n",
    "\n",
    "        f1 = f1_score(Y_true, Y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred, labels=[0, 1]).ravel()\n",
    "        tnr = tn / (tn + fp)\n",
    "        fpr = fp / (fp + tn)\n",
    "        fnr = fn / (fn + tp)\n",
    "        tpr = tp / (tp + fn)\n",
    "\n",
    "        row = {'Model': model_name, 'Time': '{:04.3f}'.format(prediction_time),\n",
    "               'Samples': Y_pred.shape[0], 'DDOS%': ddos_rate, 'Accuracy': '{:05.4f}'.format(accuracy), 'F1Score': '{:05.4f}'.format(f1),\n",
    "               'TPR': '{:05.4f}'.format(tpr), 'FPR': '{:05.4f}'.format(fpr), 'TNR': '{:05.4f}'.format(tnr), 'FNR': '{:05.4f}'.format(fnr), 'Source': data_source}\n",
    "\n",
    "    pprint.pprint(row, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement a Neural Network model\n",
    "Finalise the CNN model by using all the four arguments of ```create_CNN_model```. You will use ALL these arguments to tune the model afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MLP_model(optimizer=Adam, dense_layers=4, hidden_units=2, learning_rate = 0.001):\n",
    "    model = Sequential(name  = \"mlp\")\n",
    "    model.add(Dense(hidden_units, input_shape=(21,), activation='relu'))\n",
    "    for layer in range(dense_layers):\n",
    "        model.add(Dense(hidden_units, activation='relu', name='hidden-fc' + str(layer)))\n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Function to create the CNN model\n",
    "def create_CNN_model(optimizer=Adam, filters = 100, kernel_size=(3,3), strides=(1,1), padding='same',learning_rate = 0.001,dropout_rate=0.1):\n",
    "    model = Sequential(name  = \"cnn\")\n",
    "\n",
    "    ### ADD YOUR CODE HERE ###\n",
    "\n",
    "    ##########################\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on static test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset_path, model_path):\n",
    "    if dataset_path is not None:\n",
    "        X_test, y_test = load_dataset(dataset_path + \"/*\" + '-test.hdf5')\n",
    "\n",
    "        if model_path == None or model_path.endswith('.h5') == False:\n",
    "                print (\"No valid model specified!\")\n",
    "                exit(-1)\n",
    "\n",
    "        if model_path is not None:\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path) \n",
    "            return\n",
    "\n",
    "        pt0 = time.time()\n",
    "        for i in range(TEST_ITERATIONS):\n",
    "            Y_pred = np.squeeze(model.predict(X_test, batch_size=16) > 0.5,axis=1)\n",
    "        pt1 = time.time()\n",
    "        prediction_time = pt1 - pt0\n",
    "\n",
    "        report_results(np.squeeze(y_test), Y_pred,  model.name, '', prediction_time/TEST_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on live traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_live(source,model_path):\n",
    "    if source is not None:\n",
    "        if source.endswith('.pcap'):\n",
    "            pcap_file = source\n",
    "            cap = pyshark.FileCapture(pcap_file)\n",
    "            data_source = pcap_file.split('/')[-1].strip()\n",
    "        else:\n",
    "            cap =  pyshark.LiveCapture(interface=source)\n",
    "            data_source = args.predict_live\n",
    "\n",
    "        print (\"Prediction on network traffic from: \", source)\n",
    "\n",
    "        if model_path is not None:\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path) \n",
    "            return\n",
    "\n",
    "        # load the labels, if available\n",
    "        labels = parse_labels('DOS2019')\n",
    "\n",
    "        if MODEL_TYPE == 'MLP':\n",
    "            MAX_FLOW_LEN=1000\n",
    "            mins, maxs = static_min_max(flatten=True,time_window=10,max_flow_len=MAX_FLOW_LEN)\n",
    "        else:\n",
    "            MAX_FLOW_LEN=10\n",
    "            mins, maxs = static_min_max(flatten=False,time_window=10,max_flow_len=MAX_FLOW_LEN)\n",
    "\n",
    "        while (True):\n",
    "            samples = process_live_traffic(cap, 'DOS2019', labels, max_flow_len=MAX_FLOW_LEN, traffic_type=\"all\",time_window=10)\n",
    "            if len(samples) > 0:\n",
    "                X,Y_true,flow_ids = dataset_to_list_of_fragments(samples)\n",
    "                if MODEL_TYPE == 'MLP':\n",
    "                    X = flatten_samples(X)\n",
    "                    X = np.array(normalize(X, mins, maxs))\n",
    "                else:\n",
    "                    X = np.array(normalize_and_padding(X, mins, maxs, MAX_FLOW_LEN))\n",
    "                if labels is not None:\n",
    "                    Y_true = np.array(Y_true)\n",
    "                else:\n",
    "                    Y_true = None\n",
    "                \n",
    "                pt0 = time.time()\n",
    "                Y_pred = np.squeeze(model.predict(X, batch_size=2048) > 0.5,axis=1)\n",
    "                pt1 = time.time()\n",
    "                prediction_time = pt1 - pt0\n",
    "\n",
    "                report_results(np.squeeze(Y_true), Y_pred,  MODEL_TYPE, '', prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning\n",
    "Define the list of hyperparameters, along with their search ranges and distributions, for the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_path, model_path):\n",
    "\n",
    "    if dataset_path is not None:\n",
    "        X_train, y_train = load_dataset(dataset_path + \"/*\" + '-train.hdf5')\n",
    "        X_val, y_val = load_dataset(dataset_path + \"/*\" + '-val.hdf5')\n",
    "\n",
    "        param_mlp_dist = {\n",
    "            'learning_rate' : uniform(0.0001, 0.001),\n",
    "            'optimizer' : [SGD,Adam],\n",
    "            'dense_layers' : randint(1,8),\n",
    "            'hidden_units': randint(16,32)\n",
    "        }\n",
    "\n",
    "        param_cnn_dist = {\n",
    "            ### ADD YOUR CODE HERE ###\n",
    "\n",
    "            ##########################\n",
    "        }\n",
    "\n",
    "        if MODEL_TYPE == 'MLP':\n",
    "            param_dist = param_mlp_dist\n",
    "            create_model = create_MLP_model\n",
    "        else:\n",
    "            param_dist = param_cnn_dist\n",
    "            create_model = create_CNN_model\n",
    "\n",
    "        model = KerasClassifier(build_fn=create_model, batch_size=100, verbose=1)\n",
    "\n",
    "        random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=5, cv=2, random_state=SEED)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "        random_search_result = random_search.fit(X_train, y_train,epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "        # Print the best parameters and corresponding accuracy\n",
    "        print(\"Best parameters found: \", random_search_result.best_params_)\n",
    "        print(\"Best cross-validated accuracy: {:.2f}\".format(random_search_result.best_score_))\n",
    "\n",
    "        # Save the best model\n",
    "        best_model = random_search.best_estimator_.model\n",
    "        if model_path is not None:\n",
    "            save_model(best_model,model_path)\n",
    "        else:\n",
    "            print (\"Invalid model path: \", model_path)\n",
    "            print (\"Model saved as: ./\" + MODEL_TYPE + '_model.h5')\n",
    "            save_model(best_model, './'+ MODEL_TYPE + '_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model\n",
    "Train the model by calling the ```train``` method with static dataset path, which depends on the ```MODEL_TYPE``` variable, as the shape of the input layer of the two models is different (vector for the MLP, array for the CNN).\n",
    "\n",
    "If you wish to export the notebook as a stand-alone Python script, replace the two arguments with ```args.train``` and ```args.model``` (see the lab on NIDS deployment for more info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train('./DOS2019_Binary_5_Attacks_'+ MODEL_TYPE, './' + MODEL_TYPE + '_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "In the following cell, you can make prediction with test traffic samples obtained from unperturbed traffic traces. This test will give you a performance baseline for your model.\n",
    "\n",
    "Also in this case, the dataset path depends on the ```MODEL_TYPE``` variable, as the shape of the input layer of the two models is different (vector for the MLP, array for the CNN).\n",
    "\n",
    "If you wish to export the notebook as a stand-alone Python script, replace the two arguments with ```args.predict``` and ```args.model``` (see the lab on NIDS deployment for more info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "predict('./DOS2019_Binary_5_Attacks_' + MODEL_TYPE, './' + MODEL_TYPE + '_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using a pcap file\n",
    "In the following cell, you can test the NIDS using traces of network traffic. \n",
    "Start with unperturbed traffic (i.e., running the code below), then use perturbed traffic traces to evaluate the robustness of the NN model to evasion AML attacks.\n",
    "\n",
    "Available perturbed traffic traces are:\n",
    "- IAT set to 0.5 seconds [ddos-chunk-short-IAT0.5.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-IAT0.5.pcap5.pcap)\n",
    "- Random packet length [ddos-chunk-short-PACKETLEN.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-PACKETLEN.pcap)\n",
    "- Random TCP Window size [ddos-chunk-short-WINSIZE.pcap](./DOS2019_Binary_5_Attacks_PCAPs/ddos-chunk-short-WINSIZE.pcap)\n",
    "\n",
    "**NOTE**: the important metric here is the FNR, which is the percentage of malicious samples that evades the NIDS.\n",
    "\n",
    "If you wish to export the notebook as a stand-alone Python script, replace the two arguments with ```args.predict_live``` and ```args.model``` (see the lab on NIDS deployment for more info).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on a pcap file\n",
    "\n",
    "### CHANGE THE PCAP PATH HERE ###\n",
    "predict_live('./DOS2019_Binary_5_Attacks_Pcaps/ddos-chunk-short.pcap','./' + MODEL_TYPE + '_model.h5')\n",
    "##########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
