{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71299b2",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "In this laboratory, you will tune the hyperparameters of an MLP model for binary network traffic classification. The MLP model returns a value between 0 and 1, which is the probability of the input flow of being malicious. \n",
    "You will tune the model using Grid and Random search and you will train it on a dataset of benign traffic and DDoS attack traffic.\n",
    "\n",
    "You will use a dataset of benign and various DDoS attacks from the CIC-DDoS2019 dataset (https://www.unb.ca/cic/datasets/ddos-2019.html).\n",
    "The network traffic has been previously pre-processed in a way that packets are grouped in bi-directional traffic flows using the 5-tuple (source IP, destination IP, source Port, destination Port, protocol). Each flow is represented with 21 packet-header features computed from max 1000 packets:\n",
    "\n",
    "| Feature nr.         | Feature Name |\n",
    "|---------------------|---------------------|\n",
    "| 00 | timestamp (mean IAT) | \n",
    "| 01 | packet_length (mean)| \n",
    "| 02 | IP_flags_df (sum) |\n",
    "| 03 | IP_flags_mf (sum) |\n",
    "| 04 | IP_flags_rb (sum) | \n",
    "| 05 | IP_frag_off (sum) |\n",
    "| 06 | protocols (mean) |\n",
    "| 07 | TCP_length (mean) |\n",
    "| 08 | TCP_flags_ack (sum) |\n",
    "| 09 | TCP_flags_cwr (sum) |\n",
    "| 10 | TCP_flags_ece (sum) |\n",
    "| 11 | TCP_flags_fin (sum) |\n",
    "| 12 | TCP_flags_push (sum) |\n",
    "| 13 | TCP_flags_res (sum) |\n",
    "| 14 | TCP_flags_reset (sum) |\n",
    "| 15 | TCP_flags_syn (sum) |\n",
    "| 16 | TCP_flags_urg (sum) |\n",
    "| 17 | TCP_window_size (mean) |\n",
    "| 18 | UDP_length (mean) |\n",
    "| 19 | ICMP_type (mean) |\n",
    "| 20 | Packets (counter)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aec2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from keras.regularizers import l1,l2\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from util_functions import *\n",
    "\n",
    "# disable GPUs for test reproducibility\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "SEED=0\n",
    "\n",
    "DATASET_FOLDER = \"./DOS2019_Binary\"\n",
    "\n",
    "# Load your dataset (for example, Iris dataset)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, y_train = load_dataset(DATASET_FOLDER + \"/*\" + '-train.hdf5')\n",
    "X_val, y_val = load_dataset(DATASET_FOLDER + \"/*\" + '-val.hdf5')\n",
    "X_test, y_test = load_dataset(DATASET_FOLDER + \"/*\" + '-test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compileModel(model,optimizer='sgd', lr=0.001):\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=lr, momentum=0.0)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "The following method defines the MLP model with configurable hyperparameters. Each hyperparameter has a default value that can be set during the tuning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the MLP model (for GridSearchCV)\n",
    "def create_model(optimizer='sgd', dense_layers=4, hidden_units=2, learning_rate = 0.001, dropout_rate=0, activation='relu'):\n",
    "    model = Sequential(name  = \"mlp\")\n",
    "    model.add(Dense(hidden_units, input_shape=(21,), activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for layer in range(dense_layers):\n",
    "        model.add(Dense(hidden_units, activation='relu', name='hidden-fc' + str(layer)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid', name='fc2'))\n",
    "    compileModel(model, optimizer,learning_rate)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "\n",
    "The code below performs hyperparameter tuning using the *grid search* strategy. Grid search is a hyperparameter tuning technique in machine learning that involves systematically searching for the optimal combination of hyperparameters by evaluating a model's performance on a grid of possible hyperparameter values. The grid of hyperparameters is already defined. \n",
    "\n",
    "Your first task is to add more parameters to tune, such as the number of **hidden units** (integers) and the **dropout rate** (floating point numbers between 0 and 1). Check the performance before and after adding the new parameters.\n",
    "\n",
    "Note the total training time and the accuracy on training and validation sets, which are printed at the bottom of the output log. Then add **early stopping**, as done in the [Regularization](./Regularisation.ipynb) demonstration. Train again and compare training time and accuracies. \n",
    "\n",
    "NOTE: pay attention to the **cv** parameter of the *GridSearchCV* method. It indicates the number of folds in the k-fold cross-validation. ```cv=k``` means that the training set is split into ```k``` folds, which are used alternatively for training and for validation. Therefore, for each combination of hyperparameters, the model is trained ```k``` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ac3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KerasClassifier based on the create_model function\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=100, verbose=1)\n",
    "\n",
    "PATIENCE = 10\n",
    "k=2 # number of folds for cross-validation\n",
    "\n",
    "# Define the hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'learning_rate' : [0.001,0.01],\n",
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "##########################\n",
    "    'optimizer' : ['sgd','adam']\n",
    "}\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=k)\n",
    "\n",
    "### ADD early stopping HERE\n",
    "\n",
    "###########################\n",
    "start_time = time.time()\n",
    "\n",
    "### ADD early stopping in the list of callbacks\n",
    "grid_result = grid.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks= [])\n",
    "stop_time = time.time()\n",
    "\n",
    "# Total training time\n",
    "print(\"Total training time (sec): \", stop_time-start_time)\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best parameters found: \", grid_result.best_params_)\n",
    "print(\"Best cross-validated accuracy: {:.2f}\".format(grid_result.best_score_))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test accuracy of the best model: {:.2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search\n",
    "In the following cell, you will implement *random search* to tune the hyperparameters of the MLP model. Instead of trying every combination of hyperparameters like with grid search, random combinations of hyperparameters are sampled from a specified distribution. The number of trials is determined by the user (parameter *n_iter*), and the results of each trial are used to guide the search towards better-performing combinations of hyperparameters.\n",
    "\n",
    "In the cell below, implement random search by reusing the code of the grid search approach above, and by taking inspiration from the implementation of the hyperparameter tuning for a [Random Forest model](./hyperparameter-tuning-RF.ipynb). Use the same list of hyperparameters as for the grid search, but use the *uniform* method for generating floating point values of **learning_rate** and **dropout_rate**, while use *randint* for generating the integer hyperparameters (hidden units). \n",
    "\n",
    "Add early-stopping and set the number of iterations to 20 or 30, to avoid waiting for a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "k=2 # number of folds for cross-validation\n",
    "PATIENCE = 10\n",
    "\n",
    "# Create a KerasClassifier based on the create_model function\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=100, verbose=1)\n",
    "\n",
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "# Total training time\n",
    "print(\"Total training time (sec): \", stop_time-start_time)\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best parameters found: \", random_search_result.best_params_)\n",
    "print(\"Best cross-validated accuracy: {:.2f}\".format(random_search_result.best_score_))\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test accuracy of the best model: {:.2f}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
