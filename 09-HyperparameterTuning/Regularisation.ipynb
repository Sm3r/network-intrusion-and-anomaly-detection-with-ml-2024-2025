{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding overfitting with regularisation\n",
    "In this experiment, our primary focus is on mitigating the issue of overfitting the training data. In the initial phase, we closely monitor the training process of a compact neural network designed for a binary classification problem on a synthetic dataset. By observing the model's performance metrics, such as loss and accuracy, on the validation dataset, we aim to assess whether the model is exhibiting signs of overfitting the training data.\n",
    "\n",
    "Subsequently, we implement various regularisation techniques to to tackle the problem of overfitting: L1 and L2 regularisation, Dropout and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "\n",
    "# disable GPUs for test reproducibility\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)\n",
    "\n",
    "# Generate synthetic data\n",
    "\n",
    "SAMPLES = 200\n",
    "# Generate synthetic data for binary classification\n",
    "X, y = make_classification(n_samples=SAMPLES, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, class_sep=1,flip_y=0.03,random_state=SEED)\n",
    "# Split data into training and testing sets\n",
    "X_train_orig, X_val_orig, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "\n",
    "# Add polynomial features\n",
    "poly_features = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X_train = poly_features.fit_transform(X_train_orig)\n",
    "X_val = poly_features.fit_transform(X_val_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot training and validation accuracy over epochs\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation loss over epochs\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim([0, 2])\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # Evaluate the model on the training data\n",
    "    loss, accuracy = model.evaluate(X_train, y_train)\n",
    "    print(f'Training Loss: {loss:.4f}')\n",
    "    print(f'Training Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Evaluate the model on the validation data\n",
    "    loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f'Validation Loss: {loss:.4f}')\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition and training\n",
    "In the following cell, we define and compile a neural network model. Then we train it for 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network model using Keras\n",
    "overfit_model = Sequential()\n",
    "overfit_model.add(Dense(24, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "overfit_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "overfit_model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save the weights for later\n",
    "model_initial_weights = overfit_model.get_weights()\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "overfit_model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = overfit_model.fit(X_train, y_train, epochs=1000, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "Let's compare the model's performance on training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(overfit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping strategy\n",
    "Instead of setting a fixed number of training epochs, one can implement an early-stopping strategy, which automatically stop the training process when the validation accuracy stops increasing or the validation loss stop decreasing. An important parameter is called **patience**, which represents the number of epochs with no improvement (on the validation set) after which the training will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "PATIENCE = 10 #number of epochs with no improvement on the validation set\n",
    "\n",
    "# Create a neural network model using Keras\n",
    "early_stopping_model = Sequential()\n",
    "early_stopping_model.add(Dense(24, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "early_stopping_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "early_stopping_model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria (you can monitor either val_loss or val_accuracy)\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=PATIENCE, verbose=1, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Re-initialize the weights of the model\n",
    "early_stopping_model.set_weights(model_initial_weights)\n",
    "# Retrain the model with early-stopping\n",
    "history = early_stopping_model.fit(X_train, y_train, epochs=1000, validation_data=(X_val, y_val), verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "Let's compare the model's performance on training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(early_stopping_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 regularisation\n",
    "Instead stopping the training process, this time we try to avoid overfitting by applying L1 and L2 regularisation. The key parameter here is \\(\\lambda\\), which controls the strength of the regularization term.\n",
    "\n",
    "**L1 regularisation**:  \\( J_W(y,\\hat{y}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda \\sum_{l=1}^{L} \\sum_{j=1}^{n^{(l)}} |w_{j}^{(l)}| \\)\n",
    "**L2 regularisation**:  \\( J_W(y,\\hat{y}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right) + \\lambda \\sum_{l=1}^{L} \\sum_{j=1}^{n^{(l)}} (w_{j}^{(l)})^2 \\)\n",
    "\n",
    "- \\( m \\) is the number of samples in a mini-batch of training samples,\n",
    "- \\( y_i \\) represents the true labels (either 0 or 1) of the \\( i \\)th sample,\n",
    "- \\( \\hat{y}_i \\) represents the predicted probability of the \\( i \\)th sample being in class 1,\n",
    "- \\( L \\) is the total number of layers in the neural network,\n",
    "- \\( n^{(l)} \\) is the total number of weights in layer \\( l \\),\n",
    "- \\( w_{j}^{(l)} \\) represents the individual weights in layer \\( l \\),\n",
    "- \\( \\lambda \\) (lambda) is the regularization parameter, controlling the strength of the L1 regularization. A higher value of \\( \\lambda \\) leads to stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1,l2\n",
    "# Create a neural network model using Keras\n",
    "l1_model = Sequential()\n",
    "\n",
    "# Use either l1 or l2 regularisation and try increasing the value of lambda\n",
    "l1_model.add(Dense(24, input_shape=(X_train.shape[1],), activation='relu',kernel_regularizer=l2(0.3)))\n",
    "l1_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "l1_model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Re-initialize the weights of the model\n",
    "l1_model.set_weights(model_initial_weights)\n",
    "# Retrain the model with early-stopping\n",
    "history = l1_model.fit(X_train, y_train, epochs=1000, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "Let's compare the model's performance on training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(l1_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Dropout randomly deactivates a fraction of neurons during training, meaning these neurons are ignored during forward and backward passes. At every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability \\(p\\) of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter \\(p\\) is called the ```dropout rate```, and it is typically set between 10\\% and 50\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "# Create a neural network model using Keras\n",
    "DROPOUT_RATE = 0.95\n",
    "\n",
    "dropout_model = Sequential()\n",
    "\n",
    "# Use either l1 or l2 regularisation and try increasing the value of lambda\n",
    "dropout_model.add(Dense(24, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "dropout_model.add(Dropout(DROPOUT_RATE))  # Dropout with a dropout rate of 0.5 (50% of neurons will be dropped out during training)\n",
    "dropout_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Re-initialize the weights of the model\n",
    "dropout_model.set_weights(model_initial_weights)\n",
    "# Retrain the model with early-stopping\n",
    "history = dropout_model.fit(X_train, y_train, epochs=1000, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "Let's compare the model's performance on training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dropout_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
