{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Unsupervised Pretraining Using Stacked Autoencoders\n",
    "This notebook illustrates the training process of a Network Intrusion Detection System (NIDS) using a dataset with a limited number of labeled samples. The approach involves initial unsupervised training of an autoencoder on the entire dataset, incorporating both labeled and unlabeled samples, where the labels are disregarded during this phase.\n",
    "\n",
    "The key strategy is to pre-train the autoencoder to learn meaningful representations from the data without relying on labeled information. Subsequently, we leverage the pre-trained encoder to construct a new neural network for building the NIDS. The rationale behind unsupervised pretraining with stacked autoencoders lies in the ability of the initial layers to autonomously capture general features and patterns present in the data. This understanding is then leveraged to enhance the NIDS's ability to detect network intrusions with a limited set of labeled examples.\n",
    "\n",
    "| <img src=\"./autoencoder_pretraining.png\" width=\"100%\">  |\n",
    "|--|\n",
    "| The two stages of unsupervised pretraining using stacked Autoencoders|\n",
    "\n",
    "\n",
    "We will use a dataset of benign and various DDoS attacks from the CIC-DDoS2019 dataset (https://www.unb.ca/cic/datasets/ddos-2019.html).\n",
    "The network traffic has been previously pre-processed in a way that packets are grouped in bi-directional traffic flows using the 5-tuple (source IP, destination IP, source Port, destination Port, protocol). Each flow is represented with 21 packet-header features computed from max 1000 packets:\n",
    "\n",
    "| Feature nr.         | Feature Name |\n",
    "|---------------------|---------------------|\n",
    "| 00 | timestamp (mean IAT) | \n",
    "| 01 | packet_length (mean)| \n",
    "| 02 | IP_flags_df (sum) |\n",
    "| 03 | IP_flags_mf (sum) |\n",
    "| 04 | IP_flags_rb (sum) | \n",
    "| 05 | IP_frag_off (sum) |\n",
    "| 06 | protocols (mean) |\n",
    "| 07 | TCP_length (mean) |\n",
    "| 08 | TCP_flags_ack (sum) |\n",
    "| 09 | TCP_flags_cwr (sum) |\n",
    "| 10 | TCP_flags_ece (sum) |\n",
    "| 11 | TCP_flags_fin (sum) |\n",
    "| 12 | TCP_flags_push (sum) |\n",
    "| 13 | TCP_flags_res (sum) |\n",
    "| 14 | TCP_flags_reset (sum) |\n",
    "| 15 | TCP_flags_syn (sum) |\n",
    "| 16 | TCP_flags_urg (sum) |\n",
    "| 17 | TCP_window_size (mean) |\n",
    "| 18 | UDP_length (mean) |\n",
    "| 19 | ICMP_type (mean) |\n",
    "| 20 | Packets (counter)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import time\n",
    "import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from util_functions import *\n",
    "DATASET_FOLDER = \"./DOS2019_Binary\"\n",
    "X_train, y_train = load_dataset(DATASET_FOLDER + \"/*\" + '-train.hdf5')\n",
    "X_val, y_val = load_dataset(DATASET_FOLDER + \"/*\" + '-val.hdf5')\n",
    "X_test, y_test = load_dataset(DATASET_FOLDER + \"/*\" + '-test.hdf5')\n",
    "\n",
    "# disable GPUs for test reproducibility\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "SEED=1\n",
    "PATIENCE = 25\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_results(Y_true, Y_pred, model_name, data_source, prediction_time):\n",
    "    ddos_rate = '{:04.3f}'.format(sum(Y_pred) / Y_pred.shape[0])\n",
    "\n",
    "    if Y_true is not None and len(Y_true.shape) > 0:  # if we have the labels, we can compute the classification accuracy\n",
    "        Y_true = Y_true.reshape((Y_true.shape[0], 1))\n",
    "        accuracy = accuracy_score(Y_true, Y_pred)\n",
    "\n",
    "        f1 = f1_score(Y_true, Y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred, labels=[0, 1]).ravel()\n",
    "        tnr = tn / (tn + fp)\n",
    "        fpr = fp / (fp + tn)\n",
    "        fnr = fn / (fn + tp)\n",
    "        tpr = tp / (tp + fn)\n",
    "\n",
    "        row = {'Model': model_name, 'Time': '{:04.3f}'.format(prediction_time),\n",
    "               'Samples': Y_pred.shape[0], 'DDOS%': ddos_rate, 'Accuracy': '{:05.4f}'.format(accuracy), 'F1Score': '{:05.4f}'.format(f1),\n",
    "               'TPR': '{:05.4f}'.format(tpr), 'FPR': '{:05.4f}'.format(fpr), 'TNR': '{:05.4f}'.format(tnr), 'FNR': '{:05.4f}'.format(fnr), 'Source': data_source}\n",
    "\n",
    "    pprint.pprint(row, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training\n",
    "In this first step, we implement and train an MLP model for binary traffic classification using supervised learning. Thus, we assume that all the samples are labelled. This will be the baseline to evaluate the pre-trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "def create_model():\n",
    "    model = Sequential(name  = \"mlp\", layers=[Input(shape=(X_train.shape[1],)),\n",
    "                                              Dense(18, activation=\"relu\"), \n",
    "                                              Dense(12, activation=\"relu\")])\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print (model.summary())\n",
    "    return model\n",
    "\n",
    "mlp_model = create_model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "start_time = time.time()\n",
    "history = mlp_model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_val, y_val), callbacks= [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model\n",
    "Y_pred = np.squeeze(mlp_model.predict(X_test, batch_size=16) > 0.5,axis=1)\n",
    "report_results(np.squeeze(y_test), Y_pred,  'MLP', '', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised pre-training\n",
    "Here we define an autoencoder, whose encoder is exactly like the previous MLP model (except for the output layer). We first perform unsupervised pre-training of the autoencoder using the whole training and validation sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the stacked autoencoder\n",
    "print (\"Shape of the samples: \", X_train.shape[1])\n",
    "\n",
    "stacked_encoder = Sequential(name='Encoder',layers=[Input(shape=(X_train.shape[1],)), \n",
    "                              Dense(18, activation=\"relu\",name='encoder1'), \n",
    "                              Dense(12, activation=\"relu\",name='encoder2')]) \n",
    "\n",
    "stacked_decoder = Sequential(name='Decoder',layers=[ Dense(18, activation=\"relu\", input_shape=[12]), \n",
    "                              Dense(X_train.shape[1], activation=\"sigmoid\") ]) \n",
    "stacked_ae = Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "# Compile the model\n",
    "stacked_ae.compile(optimizer=Adam(learning_rate=0.01), loss='mean_squared_error')\n",
    "print (stacked_encoder.summary())\n",
    "print (stacked_decoder.summary())\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "# Train the stacked autoencoder\n",
    "history = stacked_ae.fit(X_train, X_train, epochs=1000, batch_size=64, shuffle=True, validation_data=(X_val, X_val),callbacks=[early_stopping])\n",
    "stacked_ae.save(\"./stacked_ae.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the MLP model by using the pre-trained Autoencoder\n",
    "In this last step, we perform supervised training of the MLP model with only a small portion of the training set. Thus, we simulate the situation in which only part of the training set is labelled.\n",
    "To address this limitation, we re-use the pre-trained encoder of the Autoencoder to build the MLP model. During the training process, the encoder layers are not trained (they are \"frozen\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small,_,y_train_small,_ = train_test_split(X_train,y_train, train_size=0.1, shuffle=True,random_state=SEED) # simulate that only part of the training set is labelled\n",
    "X_train_small,X_val_small,y_train_small,y_val_small = train_test_split(X_train_small,y_train_small, train_size=0.9, shuffle=True,random_state=SEED) \n",
    "\n",
    "# Freeze the encoder layers\n",
    "for layer in stacked_encoder.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "mlp_pretrained = Sequential([stacked_encoder, \n",
    "                             Dense(10, activation='relu'), \n",
    "                             Dense(1, activation='sigmoid')])\n",
    "mlp_pretrained.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE, restore_best_weights=True)\n",
    "history = mlp_pretrained.fit(X_train_small, y_train_small, epochs=1000, batch_size=64, validation_data=(X_val_small, y_val_small), callbacks= [early_stopping])\n",
    "mlp_pretrained.save(\"./mlp_pretrained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the trained model\n",
    "Y_pred = np.squeeze(mlp_pretrained.predict(X_test, batch_size=16) > 0.5,axis=1)\n",
    "report_results(np.squeeze(y_test), Y_pred,  'MLP', '', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
