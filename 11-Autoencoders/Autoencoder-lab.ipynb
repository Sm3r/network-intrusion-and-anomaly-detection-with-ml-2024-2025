{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection with Stacked Autoencoders\n",
    "In this laboratory, you will implement and tune the hyperparameters of a Stacked Autoencoder for network anomaly detection. The autoencoder must be trained and validated using benign traffic samples and then tested on network anomalies (in this case, network attacks). \n",
    "You will tune the model using Grid or Random search in order to find the best hyperparameters for this task. In particular, you will focus on *learning rate*, size of the *coding layer*, *batch size* and *optimizer*.\n",
    "\n",
    "| <img src=\"./autoencoder.png\" width=\"80%\">  |\n",
    "|--|\n",
    "| Architecture of a stacked autoencoder|\n",
    "\n",
    "We will use a dataset of benign and various DDoS attacks from the CIC-DDoS2019 dataset (https://www.unb.ca/cic/datasets/ddos-2019.html).\n",
    "The network traffic has been previously pre-processed in a way that packets are grouped in bi-directional traffic flows using the 5-tuple (source IP, destination IP, source Port, destination Port, protocol). Each flow is represented with 21 packet-header features computed from max 1000 packets:\n",
    "\n",
    "| Feature nr.         | Feature Name |\n",
    "|---------------------|---------------------|\n",
    "| 00 | timestamp (mean IAT) | \n",
    "| 01 | packet_length (mean)| \n",
    "| 02 | IP_flags_df (sum) |\n",
    "| 03 | IP_flags_mf (sum) |\n",
    "| 04 | IP_flags_rb (sum) | \n",
    "| 05 | IP_frag_off (sum) |\n",
    "| 06 | protocols (mean) |\n",
    "| 07 | TCP_length (mean) |\n",
    "| 08 | TCP_flags_ack (sum) |\n",
    "| 09 | TCP_flags_cwr (sum) |\n",
    "| 10 | TCP_flags_ece (sum) |\n",
    "| 11 | TCP_flags_fin (sum) |\n",
    "| 12 | TCP_flags_push (sum) |\n",
    "| 13 | TCP_flags_res (sum) |\n",
    "| 14 | TCP_flags_reset (sum) |\n",
    "| 15 | TCP_flags_syn (sum) |\n",
    "| 16 | TCP_flags_urg (sum) |\n",
    "| 17 | TCP_window_size (mean) |\n",
    "| 18 | UDP_length (mean) |\n",
    "| 19 | ICMP_type (mean) |\n",
    "| 20 | Packets (counter)|\n",
    "\n",
    "**IMPORTANT**: The traffic features of the dataset used in this laboratory have been previously normalised between 0 and 1. Therefore, you can use the *Sigmoid* activation function in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform, randint\n",
    "from util_functions import *\n",
    "DATASET_FOLDER = \"./DOS2019_Anomaly_Flatten\"\n",
    "X_train, _ = load_dataset(DATASET_FOLDER + \"/*\" + '-benign-train.hdf5')\n",
    "X_val, _ = load_dataset(DATASET_FOLDER + \"/*\" + '-benign-val.hdf5')\n",
    "X_test, _ = load_dataset(DATASET_FOLDER + \"/*\" + '-benign-test.hdf5')\n",
    "X_test_anomalies, _ = load_dataset(DATASET_FOLDER + \"/*\" + '-anomaly-test.hdf5')\n",
    "\n",
    "SEED = 0\n",
    "PATIENCE = 25\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "Define the Stacked Autoencoder here by adding the missing hidden layers to both *encoder* and *decoder*. Keep in mind that the input shape of the decoder must be the same as the encoder's output shape (the *coding_layer_size*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_units=10,coding_layer_size=2, learning_rate = 0.001, optimizer=SGD):\n",
    "    stacked_encoder = Sequential(name='Encoder',layers=[Input(shape=(X_train.shape[1],)), \n",
    "                              ### ADD YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "                              ##########################\n",
    "                              ]) \n",
    "\n",
    "    stacked_decoder = Sequential(name='Decoder',layers=[ \n",
    "                            ### ADD YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "                            ##########################\n",
    "                            ]) \n",
    "    stacked_ae = Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "    # Compile the model\n",
    "    stacked_ae.compile(optimizer=optimizer(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    print (stacked_encoder.summary())\n",
    "    print (stacked_decoder.summary())\n",
    "    return stacked_ae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and tuning the Stacked Autoencoder for anomaly detection\n",
    "First, implement hyperparameter tuning for your autoencoder. Focus on *learning rate*, size of the *coding layer*, *batch size* and *optimizer*.\n",
    "Then, configure *early stopping* and train the autoencoder using *random search*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the stacked autoencoder\n",
    "print (\"Shape of the samples: \", X_train.shape[1])\n",
    "\n",
    "# Create a KerasClassifier based on the create_model function\n",
    "stacked_ae = KerasRegressor(build_fn=create_model, batch_size=128, verbose=1)\n",
    "\n",
    "# Define the hyperparameters to tune and their possible values\n",
    "param_dist = {\n",
    "    ### ADD YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "    ##########################\n",
    "}\n",
    "\n",
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "random_search = ### DEFINE RANDOM SEARCH HERE ###\n",
    "early_stopping = ### DEFINE EARLY STOPPING HERE\n",
    "\n",
    "# Train the stacked autoencoder\n",
    "search_result = ### RUN HYPERPARAMETER TUNING WITH RANDOM SEARCH AND EARLY STOPPING ###\n",
    "\n",
    "##########################\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best parameters found: \", search_result.best_params_)\n",
    "\n",
    "# Save the best model for later\n",
    "best_model = search_result.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the autoencoder\n",
    "In this last step, you can evaluate the autoencoder on unseen data. In particular, you can test the ability of the model to detect network anomalies and its sensitivity to benign outliers by measuring the False Positive Rate.\n",
    "\n",
    "An important parameter here is the **anomaly threshold, defined as the sum of mean and standard deviation of the reconstruction error measured on the benign validation data**.\n",
    "\n",
    "**DEFINITION**: The **standard deviation** is a measure of the amount of variability or spread in a set of data values. It indicates how much individual data points differ from the mean (average) of the data set. A low standard deviation means that the data points are close to the mean, while a high standard deviation indicates that they are spread out over a wider range of values.\n",
    "\n",
    "By setting the threshold to the mean plus the standard deviation, we aim to ensure that most benign samples (which are expected to produce low error) are classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the anomaly threshold using the erron on the validation data\n",
    "reconstructed_benign_validation = best_model.predict(X_val)\n",
    "reconstruction_error_benign_validation = np.mean(np.square(X_val - reconstructed_benign_validation), axis=1)\n",
    "# Set a threshold for anomaly detection (adjust as needed)\n",
    "anomaly_threshold = np.mean(reconstruction_error_benign_validation) + np.std(reconstruction_error_benign_validation)\n",
    "\n",
    "# Evaluate the model on unseen benign and anomalous traffic\n",
    "reconstructed_benign_test = best_model.predict(X_test)\n",
    "reconstructed_anomalies = best_model.predict(X_test_anomalies)\n",
    "\n",
    "# Calculate reconstruction errors on unseen data\n",
    "reconstruction_error_benign_test = np.mean(np.square(X_test - reconstructed_benign_test), axis=1)\n",
    "reconstruction_error_anomalies = np.mean(np.square(X_test_anomalies - reconstructed_anomalies), axis=1)\n",
    "\n",
    "# Identify anomalies\n",
    "false_positives = np.where(reconstruction_error_benign_test > anomaly_threshold)[0]\n",
    "anomalies = np.where(reconstruction_error_anomalies > anomaly_threshold)[0]\n",
    "\n",
    "# Print the indices of detected anomalies\n",
    "print(\"Detected anomalies:\", anomalies)\n",
    "print(\"Anomaly detection accuracy: \", float(len(anomalies))/X_test_anomalies.shape[0])\n",
    "print(\"False positive rate: \", float(len(false_positives))/X_test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
