{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a MLP model to solve the XOR problem\n",
    "In this notebook we show the forward and backward propagation phases in the training process of an MLP model. Our goal is to model the XOR function.\n",
    "In the XOR problem, the inputs are binary (0 or 1) and the output is also binary as shown in the following table.\n",
    "\n",
    "| Input 1 | Input 2 | XOR |\n",
    "|---------|---------|-----|\n",
    "|    0    |    0    |  0  |\n",
    "|    0    |    1    |  1  |\n",
    "|    1    |    0    |  1  |\n",
    "|    1    |    1    |  0  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation and cost functions\n",
    "First we define the activation function used in both hidden and output layers (the sigmoid), and the cost function, the Mean Squared Error (MSE). The are defined as:\n",
    "$$\\sigma(z)=\\frac{1}{1-e^{-z}}\\qquad J(y,\\hat{y})=\\frac{1}{2m}\\sum_{i=1}^m(\\hat{y}^{(i)}-y^{(i)})^2$$\n",
    "where $m$ is the number of training samples in a mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "SEED = 0\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "# Define the loss and its derivative\n",
    "def loss(y_true, y_pred, loss='mse'):\n",
    "    if loss == 'mse': #MSE\n",
    "        return (0.5 * np.mean((y_pred-y_true) ** 2))\n",
    "    else: #binary cross-entropy\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to avoid log(0)\n",
    "        return (-(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))).item()\n",
    "\n",
    "def loss_derivative(y_true, y_pred, loss='mse'):\n",
    "    if loss == 'mse': #MSE\n",
    "        return y_pred - y_true\n",
    "    else:\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip values to avoid division by zero\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron for binary classification\n",
    "The MLP model is defined in the next cell and follows the architecture depicted in the figure below, where \\(x=\\{x_1,x_2\\}\\) is an input sample consisting of 2 features.\n",
    "\n",
    "| MLP model           |\n",
    "|--------------------|\n",
    "| <img src=\"./MLP_architecture.png\" width=\"80%\">  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset (example data)\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# Corresponding target values (labels)\n",
    "Y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# Define the hyperparameters\n",
    "n0 = 2 # input size\n",
    "n1 = 3 # hidden layer size\n",
    "n2 = 1 # output size\n",
    "learning_rate = 0.1\n",
    "epochs = 20000\n",
    "\n",
    "# Initialize weights and biases with random numbers (uniform distribution)\n",
    "W1 = np.random.uniform(size=(n0, n1)) # hidden layer weights\n",
    "b1 = np.random.uniform(size=(1, n1)) # hidden layer biases\n",
    "W2 = np.random.uniform(size=(n1, n2)) # output layer weights\n",
    "b2 = np.random.uniform(size=(1, n2)) # output layer bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and backward propagations\n",
    "In the next cell, the MLP model is trained with Gradient Descent through multiple epochs of forward and backward propagations.\n",
    "For the sake of simplicity, let's consider the binary cross-entropy loss function for a single data point: $$J(y, \\hat{y}) = \\frac{1}{2}(\\hat{y}-y)^2$$\n",
    "\n",
    "\n",
    "**Compute Output Layer Error (Loss Gradient)**: For binary cross-entropy loss function, the derivative of the loss with respect to the output can be computed using the chain rule: \n",
    "    $$\\delta^{(2)} = \\frac{\\partial J(y, \\hat{y})}{\\partial z^{(2)}} =  \\frac{\\partial J(y, \\hat{y})}{\\partial\\hat{y}}\\cdot \\frac{\\partial\\hat{y}}{\\partial z^{(2)}} = (\\hat{y}-y)\\cdot \\hat{y}\\cdot(1-\\hat{y})$$\n",
    "    where $y$ is the true label (either 0 or 1), $\\hat{y} =  \\sigma(z^{(2)})$, hence its derivative is $\\hat{y}'=\\hat{y}\\cdot(1-\\hat{y})$. \n",
    "    \n",
    "**Compute Hidden Layer Error**: with similar reasoning we can demonstrate that $$\\delta^{(1)} = (\\delta^{(2)} \\cdot W^{(2)T}) \\cdot \\sigma'(z^{(1)})$$\n",
    "\n",
    "**Compute Weight Gradients**: Given that $\\delta^{(2)} = \\sigma(a^{(1)} \\cdot W^{(2)} + b^{(2)})-y$ and $\\delta^{(1)} = (\\delta^{(2)} \\cdot W^{(2)\\top}) \\cdot \\sigma'(z^{(1)})$, we apply again the chain rule to obtain the gradients of the cost function $J$ with respect of the weights:\n",
    "   $$\n",
    "   \\frac{\\partial J(y, \\hat{y})}{\\partial W^{(2)}} =  \\frac{\\partial J(y, \\hat{y})}{\\partial\\hat{y}}\\cdot \\frac{\\partial\\hat{y}}{\\partial W^{(2)}} = \\frac{\\partial J(y, \\hat{y})}{\\partial\\hat{y}}\\cdot \\frac{\\partial\\hat{y}}{\\partial z^{(2)}}\\cdot \\frac{\\partial z^{(2)}}{\\partial W^{(2)}}= \\frac{\\partial J(y, \\hat{y})}{\\partial z^{(2)}}\\cdot\\frac{\\partial z^{(2)}}{\\partial W^{(2)}} = a^{(1)\\top}\\cdot \\delta^{(2)}\n",
    "   $$\n",
    "   note that $\\frac{\\partial z^{(2)}}{\\partial W^{(2)}}=a^{(1)\\top}$.\n",
    "   $$\n",
    "    \\frac{\\partial J}{\\partial b^{(2)}} =  \\frac{\\partial J(y, \\hat{y})}{\\partial z^{(2)}}\\cdot\\frac{\\partial z^{(2)}}{\\partial b^{(2)}} = \\delta^{(2)}\n",
    "   $$\n",
    "  \n",
    "   analogously we obtain: $$\\frac{\\partial J}{\\partial W^{(1)}} = x^\\top \\cdot \\delta^{(1)}\\qquad\\frac{\\partial J}{\\partial b^{(1)}} = \\delta^{(1)}$$\n",
    "\n",
    "   which demonstrates how the output error $\\delta^{(2)}$ is back-propagated from the output layer to the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the MLP model with Mean Squared Error (MSE) loss\n",
    "Y_pred = np.zeros(Y.shape) # here we store the prediction for each sample\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)): # looping over the samples\n",
    "        x = X[i:i+1,:] #i-th sample\n",
    "        y = Y[i]       #i-th label\n",
    "        # Forward Propagation\n",
    "        z1 = np.dot(x,W1) + b1 # matrix multiplication between the input and the hidden layer weights\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(a1, W2) + b2 # matrix multiplication between the hidden layer activations and the output layer weights\n",
    "        y_pred = sigmoid(z2)\n",
    "        Y_pred[i] = y_pred\n",
    "        \n",
    "        # Compute the Mean Squared Error (MSE) loss\n",
    "        J = loss(y, y_pred)\n",
    "        \n",
    "        # Backward Propagation\n",
    "        # Compute the gradients using MSE loss derivative and sigmoid derivative\n",
    "        delta2 = loss_derivative(y, y_pred) * sigmoid_derivative(y_pred)\n",
    "        delta1 = delta2.dot(W2.T) * sigmoid_derivative(sigmoid(z1)) # we put sigmoid(z1) instead of a1 to be consistent with the above explanation\n",
    "\n",
    "        dJdW2 = a1.T.dot(delta2)\n",
    "        dJdW1 = x.T.dot(delta1)\n",
    "\n",
    "        # Update the weights and biases using gradients\n",
    "        W2 = W2 - learning_rate * dJdW2\n",
    "        b2 = b2 - learning_rate * delta2 \n",
    "        \n",
    "        W1 = W1 - learning_rate * dJdW1\n",
    "        b1 = b1 - learning_rate * delta1 \n",
    "    \n",
    "    # Print the loss at every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {J:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(X_test):\n",
    "    Y_pred = [] # here we store the prediction for each sample\n",
    "    for i in range(len(X_test)): # looping over the samples\n",
    "        x = X_test[i:i+1,:] #i-th sample\n",
    "        # Forward Propagation\n",
    "        z1 = np.dot(x,W1) + b1 # matrix multiplication between the input and the hidden layer weights\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(a1, W2) + b2 # matrix multiplication between the hidden layer activations and the output layer weights\n",
    "        y_pred = sigmoid(z2)\n",
    "        Y_pred.append(y_pred)\n",
    "    return np.array(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final predictions after training\n",
    "print(\"Final Predictions:\")\n",
    "print(model_predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Decision Boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(-0.2, 1.2, 400), np.linspace(-0.2, 1.2, 400))\n",
    "xx_test = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "zz_nn = model_predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Neural Network Decision Boundary\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(xx, yy, zz_nn, levels=[0, 0.5, 1], cmap='RdBu', alpha=0.4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap='coolwarm', edgecolors='k', s=100)\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.title('Neural Network Decision Boundary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
