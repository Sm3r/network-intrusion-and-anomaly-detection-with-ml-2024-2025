{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification: overfitting and early stopping\n",
    "In this experiment, our primary focus is on mitigating the issue of overfitting in the training data. In the initial phase, we closely monitor the training process of a compact neural network designed for a binary classification problem on a synthetic dataset. By observing the model's performance metrics, such as loss and accuracy, on the validation dataset, we aim to assess whether the model is exhibiting signs of overfitting the training data.\n",
    "\n",
    "Subsequently, we implement an early stopping technique to tackle the problem of overfitting. Early stopping is a preventive measure used during training to halt the process when the model's performance on the validation dataset ceases to improve. This approach helps us maintain a balance between learning from the training data and generalizing to unseen data, mitigating the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Roberto Doriguzzi-Corin\n",
    "# Project: Course on Network Intrusion and Anomaly Detection with Machine Learning\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import make_classification\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "\n",
    "# disable GPUs for test reproducibility\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "\n",
    "SAMPLES = 200\n",
    "# Generate synthetic data for binary classification\n",
    "X, y = make_classification(n_samples=SAMPLES, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, class_sep=1,flip_y=0.03,random_state=SEED)\n",
    "# Split data into training and testing sets\n",
    "X_train_orig, X_val_orig, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "\n",
    "# Add polynomial features\n",
    "poly_features = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X_train = poly_features.fit_transform(X_train_orig)\n",
    "X_val = poly_features.fit_transform(X_val_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition and training\n",
    "In the following cell, we define and compile a neural network model. Then we train it for 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network model using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save the weights for later\n",
    "model_initial_weights = model.get_weights()\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training log analysis\n",
    "By looking at the plots of accuracy and loss, one can understand whether the model overfits the training data, and whether the training process should have been stopped earlier. For instance, one can check where the validation loss start increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss over epochs\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 2])\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundary\n",
    "Let's check the decision boundary and how it fits the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Transform meshgrid points into polynomial features\n",
    "xx_poly = poly_features.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = model.predict(xx_poly)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_train_orig[:, 0], X_train_orig[:, 1],  c=y_train, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Data Points')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print(f'Training Loss: {loss:.4f}')\n",
    "print(f'Training Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundary on unseen data\n",
    "Let's check the decision boundary and how it splits the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Transform meshgrid points into polynomial features\n",
    "xx_poly = poly_features.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = model.predict(xx_poly)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_val_orig[:, 0], X_val_orig[:, 1],  c=y_val, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Data Points')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss:.4f}')\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping strategy\n",
    "Instead of setting a fixed number of training epochs, one can implement an early-stopping strategy, which automatically stop the training process when the validation accuracy stops increasing or the validation loss stop decreasing. An important parameter is called **patience**, which represents the number of epochs with no improvement (on the validation set) after which the training will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "PATIENCE = 10 #number of epochs with no improvement on the validation set\n",
    "\n",
    "# Define early stopping criteria (you can monitor either val_loss or val_accuracy)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Re-initialize the weights of the model\n",
    "model.set_weights(model_initial_weights)\n",
    "# Retrain the model with early-stopping\n",
    "history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_val, y_val), verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training log analysis\n",
    "By looking at the plots of accuracy and loss, now we can see where the training process has been stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss over epochs\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 2])\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Transform meshgrid points into polynomial features\n",
    "xx_poly = poly_features.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = model.predict(xx_poly)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_train_orig[:, 0], X_train_orig[:, 1],  c=y_train, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Data Points')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print(f'Training Loss: {loss:.4f}')\n",
    "print(f'Training Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary as a single curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "h = .02  # Step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Transform meshgrid points into polynomial features\n",
    "xx_poly = poly_features.transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Predict probabilities for each point on the meshgrid\n",
    "Z = model.predict(xx_poly)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the contour line representing the decision boundary (where probability is 0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black')\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(X_val_orig[:, 0], X_val_orig[:, 1],  c=y_val, edgecolors='k', cmap='viridis')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Data Points')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {loss:.4f}')\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
